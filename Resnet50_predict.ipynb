{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Setup (classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "! pip3 install pyngrok nest_asyncio fastapi uvicorn loguru\n",
    "### running as a service\n",
    "\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import json\n",
    "from PIL import ImageDraw, ImageFont\n",
    "from matplotlib import patches, patheffects\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import time\n",
    "import os\n",
    "import PIL.Image as Image\n",
    "\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n"
     ]
    }
   ],
   "source": [
    "! ls -l /home/ngailam_ho/content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "classes_path = \"/root/.cache/kagglehub/datasets/jutrera/stanford-car-dataset-by-classes-folder/versions/2/car_data/car_data\"\n",
    "\n",
    "train_annos_path = classes_path\n",
    "test_annos_path = classes_path\n",
    "#classes_path = 'devkit/cars_meta.mat' names.csv\n",
    "#classes_path = \"/root/.cache/kagglehub/datasets/jutrera/stanford-car-dataset-by-classes-folder/versions/2/car_data/car_data\"\n",
    "\n",
    "def get_labels(annos_path, classes_path):\n",
    "    car_annos = sio.loadmat(path + annos_path)\n",
    "    car_meta = sio.loadmat(path + classes_path)\n",
    "    annotations = car_annos[\"annotations\"][0,:]\n",
    "    nclasses = len(car_meta[\"class_names\"][0])\n",
    "    class_names = dict(zip(range(1,nclasses),[c[0] for c in car_meta[\"class_names\"][0]]))\n",
    "\n",
    "    labelled_images = {}\n",
    "    dataset = []\n",
    "    for i,arr in enumerate(annotations):\n",
    "        # the last entry in the row is the image name\n",
    "        # The rest is the data, first bbox, then classid\n",
    "        dataset.append([y[0][0] for y in arr][0:5]+[arr[5][0]])\n",
    "    # Convert to a DataFrame, and specify the column names\n",
    "    temp_df = pd.DataFrame(dataset,\n",
    "                      columns =['BBOX_X1','BBOX_Y1','BBOX_X2','BBOX_Y2','ClassID','filename'])\n",
    "\n",
    "    temp_df = temp_df.assign(ClassName=temp_df.ClassID.map(dict(class_names)))\n",
    "    temp_df.columns = ['bbox_x1','bbox_y1','bbox_x2','bbox_y2','class_id','filename', 'class_name']\n",
    "    return temp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## setup training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, n_epochs = 5):\n",
    "    assert(0)\n",
    "    '''\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    test_accuracies = []\n",
    "    # set the model to train mode initially\n",
    "    model.train()\n",
    "    model=model.to('cuda')\n",
    "    for epoch in range(n_epochs):\n",
    "        since = time.time()\n",
    "        running_loss = 0.0\n",
    "        running_correct = 0.0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "\n",
    "            # get the inputs and assign them to cuda\n",
    "            inputs, labels = data\n",
    "            \n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            optimizer.zero_grad()\n",
    "             # forward + backward + optimize\n",
    "                \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # calculate the loss/acc later\n",
    "            running_loss += loss.item()\n",
    "            running_correct += (labels==predicted).sum().item()\n",
    "\n",
    "        epoch_duration = time.time()-since\n",
    "        epoch_loss = running_loss/len(trainloader)\n",
    "        epoch_acc = 100/32*running_correct/len(trainloader)\n",
    "        print(\"Epoch %s, duration: %d s, loss: %.4f, acc: %.4f\" % (epoch+1, epoch_duration, epoch_loss, epoch_acc))\n",
    "        \n",
    "        losses.append(epoch_loss)\n",
    "        accuracies.append(epoch_acc)\n",
    "        model.eval()\n",
    "        test_acc = eval_model(model)\n",
    "        test_accuracies.append(test_acc)\n",
    "        \n",
    "        # re-set the model to train mode after validating\n",
    "        model.train()\n",
    "        scheduler.step(test_acc)\n",
    "        since = time.time()\n",
    "    print('Finished Training')\n",
    "    '''\n",
    "    assert(0)    \n",
    "    return model, losses, accuracies, test_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_model(model):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            images, labels = data\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model_ft(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        test_acc = 100.0 * correct / total\n",
    "    print('Accuracy of the network on the test images: %0.2f %%' % (test_acc))\n",
    "    return test_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA : cuda:0\n",
      "NVIDIA GeForce RTX 2080 Ti\n",
      "root : ../StanfordCars/data/stanford-cars-dataset/data/car_data/car_data//train \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using CUDA : {device}\")\n",
    "print(torch.cuda.get_device_name(device))\n",
    "\n",
    "\n",
    "\n",
    "dataset_dir = \"../StanfordCars/data/stanford-cars-dataset/data/car_data/car_data/\"\n",
    "\n",
    "train_tfms = transforms.Compose([transforms.Resize((400, 400)),\n",
    "                                 transforms.RandomHorizontalFlip(),\n",
    "                                 transforms.RandomRotation(15),\n",
    "                                 transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "test_tfms = transforms.Compose([transforms.Resize((400, 400)),\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "print(f\"root : {dataset_dir}/train \")\n",
    "dataset = torchvision.datasets.ImageFolder(root = f\"{dataset_dir}/\",\n",
    "                                           transform = train_tfms)\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size = 32,\n",
    "                                          shuffle=True, num_workers = 2)\n",
    "\n",
    "dataset2 = torchvision.datasets.ImageFolder(root =  f\"{dataset_dir}/\",\n",
    "                                            transform = test_tfms)\n",
    "testloader = torch.utils.data.DataLoader(dataset2, batch_size = 32,\n",
    "                                         shuffle=False, num_workers = 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> torch.load('Rasnet50.pth') \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /home/ngailam_ho/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 97.8M/97.8M [00:00<00:00, 116MB/s]\n"
     ]
    }
   ],
   "source": [
    "#### LOAD MODEL\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "print(f\"\\n\\n>>> torch.load('Rasnet50.pth') \")\n",
    "\n",
    "# Step 1: Initialize model with the best available weights\n",
    "weights = ResNet50_Weights.DEFAULT\n",
    "\n",
    "#preprocess = torch.load('Rasnet50.pth', weights_only=True)\n",
    "model = resnet50( weights=weights)\n",
    "model.eval()\n",
    "\n",
    "# Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n\\n>>> img_classify(..) \")\n",
    "\n",
    "# Make predictions on a sample image\n",
    "def img_classify(image_path, img_height=128, img_width=128):\n",
    "    # image_path = \"images.jpg\"\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(img_height, img_width))\n",
    "    #img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    #img_array = tf.expand_dims(img_array, 0)  # Create a batch\n",
    "    img_tensor = transform(img).unsqueeze(0)  # Add a batch dimension\n",
    "\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(img_tensor)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "    # Get the class name corresponding to the predicted index\n",
    "    class_names = ['class1', 'class2', 'class3', 'class4', 'class5', 'class6']\n",
    "    #print(f\"# img_classify(...) :   '{image_path}' ?? --> CLASS:{predicted.item()} \")\n",
    "    return predicted.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fastapi in /home/ngailam_ho/anaconda3_3.9.13/lib/python3.9/site-packages (0.115.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ngailam_ho/anaconda3_3.9.13/lib/python3.9/site-packages (from fastapi) (4.12.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /home/ngailam_ho/anaconda3_3.9.13/lib/python3.9/site-packages (from fastapi) (2.10.1)\n",
      "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /home/ngailam_ho/anaconda3_3.9.13/lib/python3.9/site-packages (from fastapi) (0.41.3)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/ngailam_ho/anaconda3_3.9.13/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ngailam_ho/anaconda3_3.9.13/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /home/ngailam_ho/anaconda3_3.9.13/lib/python3.9/site-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.5.0)\n",
      "Requirement already satisfied: idna>=2.8 in /home/ngailam_ho/anaconda3_3.9.13/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (3.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ngailam_ho/anaconda3_3.9.13/lib/python3.9/site-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.0)\n",
      "ERROR: unknown command \"pip\"\n",
      "\n",
      "\n",
      ">>> Setting up  \n",
      "Running in jupiter notebook?\n",
      "To use the fastapi command, please install \"fastapi[standard]\":\n",
      "\n",
      "\tpip install \"fastapi[standard]\"\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ngailam_ho/anaconda3/bin/fastapi\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/home/ngailam_ho/anaconda3/lib/python3.9/site-packages/fastapi/cli.py\", line 12, in main\n",
      "    raise RuntimeError(message)  # noqa: B904\n",
      "RuntimeError: To use the fastapi command, please install \"fastapi[standard]\":\n",
      "\n",
      "\tpip install \"fastapi[standard]\"\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_9547/3017181376.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running in jupiter notebook?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fastapi dev'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m### RUNNING IN IPYTHON3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "!pip3 install fastapi\n",
    "!pip3 pip install \"uvicorn[standard]\"\n",
    "\n",
    "print(f\"\\n\\n>>> Setting up  \")\n",
    "\n",
    "## are you running in ipython3 or jupiter notebook\n",
    "if get_ipython().__class__.__name__ == 'ZMQInteractiveShell':\n",
    "    print(\"Running in jupiter notebook?\")\n",
    "    !fastapi dev APIserver.py\n",
    "    assert (False)\n",
    "else:\n",
    "    ### RUNNING IN IPYTHON3, Serving at: http://127.0.0.1:8000    \n",
    "    !fastapi dev APIserver.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " ## Evaluation (Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file: 00490.jpg  CORRECT PREDICTED: --> 'Audi S4 Sedan 2007' : (1/490= 0.20) %\n",
      "file: 00633.jpg  CORRECT PREDICTED: --> 'Audi 100 Wagon 1994' : (2/633= 0.32) %\n",
      "file: 00641.jpg  CORRECT PREDICTED: --> 'Audi 100 Wagon 1994' : (3/641= 0.47) %\n",
      "file: 00649.jpg  CORRECT PREDICTED: --> 'Audi 100 Wagon 1994' : (4/649= 0.62) %\n",
      "file: 00653.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (5/653= 0.77) %\n",
      "file: 00654.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (6/654= 0.92) %\n",
      "file: 00656.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (7/656= 1.07) %\n",
      "file: 00661.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (8/661= 1.21) %\n",
      "file: 00668.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (9/668= 1.35) %\n",
      "file: 00670.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (10/670= 1.49) %\n",
      "file: 00673.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (11/673= 1.63) %\n",
      "file: 00675.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (12/675= 1.78) %\n",
      "file: 00677.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (13/677= 1.92) %\n",
      "file: 00703.jpg  CORRECT PREDICTED: --> 'Audi S5 Coupe 2012' : (14/703= 1.99) %\n",
      "file: 00719.jpg  CORRECT PREDICTED: --> 'Audi S5 Coupe 2012' : (15/719= 2.09) %\n",
      "file: 00724.jpg  CORRECT PREDICTED: --> 'Audi S5 Coupe 2012' : (16/724= 2.21) %\n",
      "file: 00815.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (17/815= 2.09) %\n",
      "file: 00816.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (18/816= 2.21) %\n",
      "file: 00821.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (19/821= 2.31) %\n",
      "file: 00822.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (20/822= 2.43) %\n",
      "file: 00824.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (21/824= 2.55) %\n",
      "file: 00827.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (22/827= 2.66) %\n",
      "file: 00828.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (23/828= 2.78) %\n",
      "file: 00829.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (24/829= 2.90) %\n",
      "file: 00832.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (25/832= 3.00) %\n",
      "file: 00833.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (26/833= 3.12) %\n",
      "file: 00834.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (27/834= 3.24) %\n",
      "file: 00836.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (28/836= 3.35) %\n",
      "file: 00838.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (29/838= 3.46) %\n",
      "file: 00845.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (30/845= 3.55) %\n",
      "file: 00848.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (31/848= 3.66) %\n",
      "file: 00849.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (32/849= 3.77) %\n",
      "file: 00850.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (33/850= 3.88) %\n",
      "file: 00851.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (34/851= 4.00) %\n",
      "file: 00653.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (35/8453= 0.41) %\n",
      "file: 00654.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (36/8454= 0.43) %\n",
      "file: 00656.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (37/8456= 0.44) %\n",
      "file: 00661.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (38/8461= 0.45) %\n",
      "file: 00668.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (39/8468= 0.46) %\n",
      "file: 00670.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (40/8470= 0.47) %\n",
      "file: 00673.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (41/8473= 0.48) %\n",
      "file: 00675.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (42/8475= 0.50) %\n",
      "file: 00677.jpg  CORRECT PREDICTED: --> 'Aston Martin Virage Coupe 2012' : (43/8477= 0.51) %\n",
      "file: 00633.jpg  CORRECT PREDICTED: --> 'Audi 100 Wagon 1994' : (44/8553= 0.51) %\n",
      "file: 00641.jpg  CORRECT PREDICTED: --> 'Audi 100 Wagon 1994' : (45/8561= 0.53) %\n",
      "file: 00649.jpg  CORRECT PREDICTED: --> 'Audi 100 Wagon 1994' : (46/8569= 0.54) %\n",
      "file: 00490.jpg  CORRECT PREDICTED: --> 'Audi S4 Sedan 2007' : (47/8701= 0.54) %\n",
      "file: 00703.jpg  CORRECT PREDICTED: --> 'Audi S5 Coupe 2012' : (48/8831= 0.54) %\n",
      "file: 00719.jpg  CORRECT PREDICTED: --> 'Audi S5 Coupe 2012' : (49/8847= 0.55) %\n",
      "file: 00724.jpg  CORRECT PREDICTED: --> 'Audi S5 Coupe 2012' : (50/8852= 0.56) %\n",
      "file: 00815.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (51/8990= 0.57) %\n",
      "file: 00816.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (52/8991= 0.58) %\n",
      "file: 00821.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (53/8996= 0.59) %\n",
      "file: 00822.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (54/8997= 0.60) %\n",
      "file: 00824.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (55/8999= 0.61) %\n",
      "file: 00827.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (56/9002= 0.62) %\n",
      "file: 00828.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (57/9003= 0.63) %\n",
      "file: 00829.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (58/9004= 0.64) %\n",
      "file: 00832.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (59/9007= 0.66) %\n",
      "file: 00833.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (60/9008= 0.67) %\n",
      "file: 00834.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (61/9009= 0.68) %\n",
      "file: 00836.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (62/9011= 0.69) %\n",
      "file: 00838.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (63/9013= 0.70) %\n",
      "file: 00845.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (64/9020= 0.71) %\n",
      "file: 00848.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (65/9023= 0.72) %\n",
      "file: 00849.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (66/9024= 0.73) %\n",
      "file: 00850.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (67/9025= 0.74) %\n",
      "file: 00851.jpg  CORRECT PREDICTED: --> 'Audi TTS Coupe 2012' : (68/9026= 0.75) %\n",
      " Accuracy: 68 / 16089 = 0.4226490148548698 %\n"
     ]
    }
   ],
   "source": [
    "#! find   /media/ngailam_ho/sdb/HTX_StanfordCars | grep -v git | head -50\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "\n",
    "test_df = pd.read_csv(\"testing_labels.csv\")\n",
    "all_models = sorted( test_df['Cars'].unique())\n",
    "models,makes,years = dict(),dict(),dict()\n",
    "\n",
    "for i,row in test_df.iterrows():\n",
    "    j = row['testing_num']\n",
    "    models[j], makes[j], years[j] = row['Cars'], row['Make'], row['Year']\n",
    "\n",
    "correct,total=0,0\n",
    "\n",
    "for root, dirs, files in sorted( os.walk( 'cars_test/cars_test', topdown=False)):\n",
    "    for file in sorted(files):\n",
    "        total = total+1\n",
    "        img_path = os.path.join( root, file)\n",
    "        testfilenum = int( file.replace(\".jpg\",\"\"))\n",
    "        predicted = img_classify( img_path)\n",
    "        \n",
    "        if testfilenum in models and f'{models[predicted]}'==f'{models[testfilenum]}':\n",
    "            correct = correct+1\n",
    "\n",
    "print(f\" Accuracy: {correct} / {total} = {100 * correct / total} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy: 68 / 16089 = 0.4226490148548698 %\n"
     ]
    }
   ],
   "source": [
    "print(f\" Accuracy: {correct} / {total} = {100 * correct / total} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 31559,
     "sourceId": 46697,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30302,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
